{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "random-shell",
   "metadata": {},
   "source": [
    "## SpeechRecognition Python library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-liquid",
   "metadata": {},
   "source": [
    "O reconhecimento automático de voz é um grande desafio. \n",
    "\n",
    "Não faltam empresas e instituições de pesquisa trabalhando em bibliotecas para ajudar a resolvê-lo. \n",
    "\n",
    "Há a biblioteca Sphinx da Carnegie Mellon University, Kaldi, SpeechRecognition e muito mais. \n",
    "\n",
    "Alguns têm recursos mais robustos do que outros, mas todos têm o mesmo objetivo de transcrever arquivos de áudio para texto. \n",
    "\n",
    "Vamos nos concentrar na biblioteca SpeechRecognition por causa de sua baixa barreira de entrada e sua compatibilidade com muitas APIs de reconhecimento de voz disponíveis que veremos em breve.\n",
    "\n",
    "Para acessar a classe Recognizer, primeiro importaremos o módulo SpeechRecognition como a abreviatura sr. Em seguida, criaremos uma instância da classe Recognizer chamando-a de sr e atribuindo a uma variável, recognizer. \n",
    "\n",
    "Por fim, definiremos o limite de energy_threshold para 300. O limite energy_threshold pode ser considerado como o volume do áudio, que é considerado fala. \n",
    "\n",
    "Valores abaixo do limite são considerados silêncio, valores acima são considerados fala. Uma sala silenciosa é normalmente entre 0 e 100. \n",
    "\n",
    "A documentação da SpeechRecognition recomenda 300 como um valor inicial que cobre a maioria dos arquivos de fala. O valor do limite de energia será ajustado automaticamente conforme o reconhecedor escuta um arquivo de áudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "typical-squad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T15:46:12.570585Z",
     "start_time": "2021-10-21T15:46:12.558645Z"
    }
   },
   "outputs": [],
   "source": [
    "import wave\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "foreign-boulder",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T15:52:28.594469Z",
     "start_time": "2021-10-21T15:52:27.363582Z"
    }
   },
   "outputs": [],
   "source": [
    "audio_file = sr.AudioFile('dataset/clean-support-call.wav')\n",
    "\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "recognizer.energy_threshold = 300\n",
    "\n",
    "with audio_file as source:\n",
    "    audio_file = recognizer.record(source)\n",
    "\n",
    "text = recognizer.recognize_google(audio_file,\n",
    "                           language = \"en-US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "furnished-manufacturer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T15:52:28.750214Z",
     "start_time": "2021-10-21T15:52:28.737231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello I'd like to get some help setting up my account please\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-entrance",
   "metadata": {},
   "source": [
    "## Reading audio files with SpeechRecognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-founder",
   "metadata": {},
   "source": [
    "Felizmente, a biblioteca SpeechRecognition tem uma classe interna, AudioFile, junto com outro método útil na classe Recognizer, record. \n",
    "\n",
    "Podemos usá-los para cuidar do pré-processamento para nós. \n",
    "\n",
    "Para começar, importamos a biblioteca SpeechRecognition e instanciamos uma instância do recognizer como antes. Então, para ler nosso arquivo de áudio, acessamos a classe AudioFile e passamos nosso nome de arquivo de áudio e o salvamos em uma variável. \n",
    "\n",
    "Nesse caso, nossa variável AudioFile é chamada de chamada de clean_support_call. Agora, se verificarmos o tipo de chamada de suporte limpa, podemos ver que é uma instância de AudioFile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ordinary-reader",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T15:55:45.730197Z",
     "start_time": "2021-10-21T15:55:45.726242Z"
    }
   },
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "chinese-confidentiality",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T15:56:24.978669Z",
     "start_time": "2021-10-21T15:56:24.973683Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_support_call = sr.AudioFile('dataset/clean-support-call.wav')\n",
    "recognizer = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "optical-doctor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T15:56:33.419359Z",
     "start_time": "2021-10-21T15:56:33.412353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speech_recognition.AudioFile"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(clean_support_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-banking",
   "metadata": {},
   "source": [
    "Nossa variável clean_support_call é atualmente do tipo AudioFile. \n",
    "\n",
    "Para convertê-la para o tipo de dados de áudio, podemos usar o método source da classe do recognizer. \n",
    "\n",
    "* Usamos um gerenciador de contexto, também conhecido como with, para abrir e ler o arquivo de áudio clean_support_call como source. \n",
    "* Em seguida, criamos o clean_support_call_audio usando o método record e transmitindo-o a source. \n",
    "* Agora, antes de chamarmos o recognizer do google novamente, vamos verificar o tipo de clean_support_call_audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "alive-raising",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:01:50.916837Z",
     "start_time": "2021-10-21T16:01:50.891905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speech_recognition.AudioData"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with clean_support_call as source:\n",
    "    clean_support_call_audio = recognizer.record(source)\n",
    "    \n",
    "type(clean_support_call_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mathematical-thursday",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:01:54.740870Z",
     "start_time": "2021-10-21T16:01:53.476183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello I'd like to get some help setting up my account please\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = recognizer.recognize_google(audio_data=clean_support_call_audio,\n",
    "                           language = \"en-US\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "documented-companion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:02:53.331206Z",
     "start_time": "2021-10-21T16:02:53.312286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nwith clean_support_call as source:\\n    clean_support_call_audio = recognizer.record(source,\\n                                                duration=None,\\n                                                offset = None) \\n    \\nwith clean_support_call as source:\\n    clean_support_call_audio = recognizer.record(source,\\n                                                duration=2.0) \\n                                                \\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "with clean_support_call as source:\n",
    "    clean_support_call_audio = recognizer.record(source,\n",
    "                                                duration=None,\n",
    "                                                offset = None) \n",
    "    \n",
    "with clean_support_call as source:\n",
    "    clean_support_call_audio = recognizer.record(source,\n",
    "                                                duration=2.0) \n",
    "                                                \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-poster",
   "metadata": {},
   "source": [
    "## Dealing with different kinds of audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-absence",
   "metadata": {},
   "source": [
    "Embora SpeechRecognition seja capaz de transcrever áudio, ele não sabe necessariamente que tipo de áudio está transcrevendo. Por exemplo, se você passar ao reconhecedor um arquivo de áudio com fala em japonês, mas a tag de idioma for inglês dos EUA, você receberá de volta o áudio em japonês em caracteres do inglês."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "illegal-check",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T17:49:20.769586Z",
     "start_time": "2021-10-21T17:49:20.750634Z"
    }
   },
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "changed-potato",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T17:49:21.363289Z",
     "start_time": "2021-10-21T17:49:21.352287Z"
    }
   },
   "outputs": [],
   "source": [
    "japanese_gm = sr.AudioFile('dataset/good-morning-japanense.wav')\n",
    "recognizer = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "finished-lecture",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T17:49:22.095004Z",
     "start_time": "2021-10-21T17:49:22.078050Z"
    }
   },
   "outputs": [],
   "source": [
    "with japanese_gm as source:\n",
    "    japanese_gm_audio = recognizer.record(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "equivalent-reward",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T17:49:44.802864Z",
     "start_time": "2021-10-21T17:49:22.800812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ohayo gozaimasu'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = recognizer.recognize_google(audio_data=japanese_gm_audio,\n",
    "                           language = \"en-US\")\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-medium",
   "metadata": {},
   "source": [
    "E se você passar o mesmo arquivo de áudio com a tag de idioma definida como ja para japonês, você obterá o áudio transcrito em caracteres japoneses. \n",
    "\n",
    "É importante observar que a biblioteca SpeechRecognition não detecta idiomas automaticamente. Portanto, você terá que garantir que esse parâmetro seja definido manualmente e que a API que você está usando tenha a capacidade de transcrever o idioma em que seus arquivos de áudio estão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "complimentary-participation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T17:50:41.864660Z",
     "start_time": "2021-10-21T17:50:34.263611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'おはようございます'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = recognizer.recognize_google(audio_data=japanese_gm_audio,\n",
    "                           language = \"ja\")\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-thirty",
   "metadata": {},
   "source": [
    "Se passarmos ao SpeechRecogition um arquivo de áudio de um leopardo rugindo, ele retornará um erro de valor desconhecido porque nenhuma fala foi detectada. \n",
    "\n",
    "O que também faz sentido porque o rugido de um leopardo, embora muito legal, não é fala humana.\n",
    "\n",
    "Podemos evitar erros usando o parâmetro show all. O parâmetro show all mostra uma lista de todas as possíveis transcrições da função recognizer. No caso de nosso rugido de leopardo, a lista volta vazia, mas evitamos gerar um erro. No caso do nosso arquivo japonês, você pode ver as diferentes transcrições possíveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "institutional-david",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T17:52:41.999361Z",
     "start_time": "2021-10-21T17:52:41.991409Z"
    }
   },
   "outputs": [],
   "source": [
    "leopard_roar = sr.AudioFile('dataset/leopard.wav')\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "\n",
    "with leopard_roar as source:\n",
    "    leopard_roar_audio = recognizer.record(source)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "streaming-surgery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T17:54:00.453764Z",
     "start_time": "2021-10-21T17:53:59.486281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = recognizer.recognize_google(leopard_roar_audio,show_all=True)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "recognized-seventh",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T17:55:53.391405Z",
     "start_time": "2021-10-21T17:55:52.425771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alternative': [{'transcript': 'ohayo gozaimasu', 'confidence': 0.98762906}], 'final': True}\n"
     ]
    }
   ],
   "source": [
    "text = recognizer.recognize_google(japanese_gm_audio,\n",
    "                                   language = \"en-US\",\n",
    "                                   show_all=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-spyware",
   "metadata": {},
   "source": [
    "Em seguida, caso o audio contenha varias pessoas falando ao mesmo tempo. \n",
    "\n",
    "A API da Web gratuita do Google transcreve a fala e a retorna como um único bloco de texto, não importa quantos falantes haja. \n",
    "\n",
    "Um único bloco de texto retornado ainda pode ser útil, no entanto, se o seu problema requer saber quem disse o quê, você pode considerar a API gratuita que estamos usando apenas como uma prova de conceito. Em seguida, use uma das versões pagas para tarefas mais complexas. \n",
    "\n",
    "O processo de separar mais de um locutor de um único arquivo de áudio é denominado diarização do locutor; no entanto, está além do escopo deste estudo.\n",
    "\n",
    "\n",
    "Para contornar o problema de vários falantes, você pode garantir que seus arquivos de áudio sejam gravados separadamente para cada falante. Em seguida, transcreva o áudio dos alto-falantes individuais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "running-listening",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T18:08:15.143000Z",
     "start_time": "2021-10-21T18:08:10.723383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"is that it doesn't recognize different speakers invoices it will just return it all as one block of text\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_speakers = sr.AudioFile('dataset/multiple-speakers-16k.wav')\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "with multiple_speakers as source:\n",
    "    multiple_speakers_audio = recognizer.record(source)\n",
    "    \n",
    "text = recognizer.recognize_google(multiple_speakers_audio, language = \"en-US\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = [sr.AudioFile('dataset/s0.wav'), sr.AudioFile('dataset/s1.wav'), sr.AudioFile('dataset/s2.wav')]\n",
    "\n",
    "for i, speaker in enumerate(speakers):\n",
    "    with speakers as source:\n",
    "        speaker_audio = recognizer.record(source)\n",
    "    print(f\"Text from speaker {i}: {recognizer.recognize_google(speaker_audio)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-button",
   "metadata": {},
   "source": [
    "Finalmente, há o problema do ruído de fundo. \n",
    "\n",
    "Uma regra prática a ser lembrada é que, se você tiver problemas para entender o que está sendo dito em um arquivo de áudio devido ao ruído de fundo, é provável que um sistema de reconhecimento de voz também tenha. \n",
    "\n",
    "Para tentar acomodar o ruído de fundo, a classe recognizer tem uma função embutida, ajuste para ruído ambiente, que leva um parâmetro, duração. \n",
    "\n",
    "A classe do reconhecedor então escuta por segundos de duração no início do arquivo de áudio e ajusta o limite de energia, ou a quantidade que a classe do reconhecedor escuta, para um nível adequado para o ruído de fundo. Quanto espaço você tem no início do seu arquivo de áudio irá ditar como você pode definir o valor da duração. A documentação da SpeechRecognition recomenda algo entre zero vírgula cinco a um segundo como um bom ponto de partida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "communist-customs",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T18:11:02.318559Z",
     "start_time": "2021-10-21T18:10:59.914995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello I'd like to get to help setting up my account\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_support_call = sr.AudioFile('dataset/2-noisy-support-call.wav')\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "with noisy_support_call as source:\n",
    "    recognizer.adjust_for_ambient_noise(source, duration=0.5)\n",
    "    noisy_support_call_audio = recognizer.record(source)\n",
    "    \n",
    "recognizer.recognize_google(noisy_support_call_audio)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

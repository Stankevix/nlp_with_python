{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "greater-google",
   "metadata": {},
   "source": [
    "# Chapter 3 - Processing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-terrain",
   "metadata": {},
   "source": [
    "## Processing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-passion",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "separate-socket",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T23:54:01.579643Z",
     "start_time": "2021-10-08T23:53:43.016024Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "apart-costa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T23:54:06.786100Z",
     "start_time": "2021-10-08T23:54:05.433036Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "grateful-investment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T23:54:07.555295Z",
     "start_time": "2021-10-08T23:54:07.544327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000001DF2C96B888>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x000001DF2C96B570>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x000001DF2C7BB180>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x000001DF2C97DEC8>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x000001DF2C97E9C8>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x000001DF2C7BB320>)]\n"
     ]
    }
   ],
   "source": [
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-lender",
   "metadata": {},
   "source": [
    "## Custom pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-consumer",
   "metadata": {},
   "source": [
    "### Simple components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "previous-delivery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:01:25.428324Z",
     "start_time": "2021-10-09T00:01:25.051153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the custom component\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(\"This document is {} tokens long.\".format(doc_length))\n",
    "    # Return the doc\n",
    "    return doc\n",
    "  \n",
    "# Load the small English model and Add the component first in the pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "#nlp.add_pipe(length_component, first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-feeling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T23:58:31.100332Z",
     "start_time": "2021-10-08T23:58:31.087368Z"
    }
   },
   "source": [
    "### Complex components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "german-marsh",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:05:17.056405Z",
     "start_time": "2021-10-09T00:05:17.048425Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the custom component\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    spans = [Span(doc, start, end, label='ANIMAL')\n",
    "             for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the component to the pipeline after the 'ner' component \n",
    "nlp.add_pipe(animal_component, after='ner')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text,ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-blackberry",
   "metadata": {},
   "source": [
    "## Scaling and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "green-willow",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:11:45.156782Z",
     "start_time": "2021-10-09T00:11:45.139827Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-lesbian",
   "metadata": {},
   "source": [
    "### Setting extension attributes (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "macro-forge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:11:46.993606Z",
     "start_time": "2021-10-09T00:11:46.976678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "# Register the Token extension attribute 'is_country' with the default value False\n",
    "Token.set_extension('is_country', default=False)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "hazardous-approach",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:13:08.756799Z",
     "start_time": "2021-10-09T00:13:08.735890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "  \n",
    "# Register the Token property extension 'reversed' with the getter get_reversed\n",
    "Token.set_extension('reversed', getter=get_reversed)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print('reversed:', token._.reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-flooring",
   "metadata": {},
   "source": [
    "### Setting extension attributes (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "exempt-underwear",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:22:07.277738Z",
     "start_time": "2021-10-09T00:22:07.274760Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "silver-mobility",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:22:36.245627Z",
     "start_time": "2021-10-09T00:22:36.227712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "# Register the Doc property extension 'has_number' with the getter get_has_number\n",
    "Doc.set_extension('has_number', getter=get_has_number,force=True)\n",
    "\n",
    "# Process the text and check the custom has_number attribute \n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print('has_number:', doc._.has_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "geological-carpet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:22:41.970709Z",
     "start_time": "2021-10-09T00:22:41.945777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return '<{tag}>{text}</{tag}>'.format(tag=tag, text=span.text)\n",
    "\n",
    "# Register the Span property extension 'to_html' with the method to_html\n",
    "Span.set_extension('to_html', method=to_html,force=True)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name 'strong'\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html('strong'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-class",
   "metadata": {},
   "source": [
    "### Entities and extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "automated-american",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:25:24.485506Z",
     "start_time": "2021-10-09T00:25:24.451596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_  in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\n",
    "        entity_text = span.text.replace(' ', '_')\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension('wikipedia_url', getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\"In over fifty years from his very first recordings right through to his last album, David Bowie was at the vanguard of contemporary culture.\")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-meeting",
   "metadata": {},
   "source": [
    "### Components with extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "pursuant-mineral",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:38:18.117143Z",
     "start_time": "2021-10-09T00:38:18.099188Z"
    }
   },
   "outputs": [],
   "source": [
    "def countries_component(doc):\n",
    "    # Create an entity Span with the label 'GPE' for all matches\n",
    "    doc.ents = [Span(doc, start, end, label='GPE')\n",
    "                for match_id, start, end in matcher(doc)]\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline\n",
    "#nlp.add_pipe(countries_component)\n",
    "#print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: capitals.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute 'capital' with the getter get_capital \n",
    "Span.set_extension('capital', getter=get_capital)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-digit",
   "metadata": {},
   "source": [
    "## Scaling and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-toner",
   "metadata": {},
   "source": [
    "### Processing streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "alien-elements",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:40:23.079526Z",
     "start_time": "2021-10-09T00:40:23.076502Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXTS = ['McDonalds is my favorite restaurant.',\n",
    " 'Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..',\n",
    " 'People really still eat McDonalds :(',\n",
    " 'The McDonalds in Spain has chicken wings. My heart is so happy ',\n",
    " '@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P',\n",
    " 'please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D',\n",
    " 'This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "beautiful-circus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:42:02.791460Z",
     "start_time": "2021-10-09T00:42:02.760544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "['open', 'BAD']\n",
      "['terrible']\n"
     ]
    }
   ],
   "source": [
    "# Process the texts and print the adjectives\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "recreational-quantity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:43:01.049597Z",
     "start_time": "2021-10-09T00:43:01.015711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(McDonalds,) () (McDonalds,) (McDonalds, Spain) (The Arch Deluxe,) () (This morning,)\n"
     ]
    }
   ],
   "source": [
    "# Process the texts and print the entities\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "amateur-conflict",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:44:36.071058Z",
     "start_time": "2021-10-09T00:44:36.041149Z"
    }
   },
   "outputs": [],
   "source": [
    "people = ['David Bowie', 'Angela Merkel', 'Lady Gaga']\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = list(nlp.pipe(people))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-jefferson",
   "metadata": {},
   "source": [
    "### Processing data with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "alert-montgomery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:49:28.735116Z",
     "start_time": "2021-10-09T00:49:28.715133Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA = [('One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.',\n",
    "  {'author': 'Franz Kafka', 'book': 'Metamorphosis'}),\n",
    " (\"I know not all that may be coming, but be it what it will, I'll go to it laughing.\",\n",
    "  {'author': 'Herman Melville', 'book': 'Moby-Dick or, The Whale'}),\n",
    " ('It was the best of times, it was the worst of times.',\n",
    "  {'author': 'Charles Dickens', 'book': 'A Tale of Two Cities'}),\n",
    " ('The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.',\n",
    "  {'author': 'Jack Kerouac', 'book': 'On the Road'}),\n",
    " ('It was a bright cold day in April, and the clocks were striking thirteen.',\n",
    "  {'author': 'George Orwell', 'book': '1984'}),\n",
    " ('Nowadays people know the price of everything and the value of nothing.',\n",
    "  {'author': 'Oscar Wilde', 'book': 'The Picture Of Dorian Gray'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "biblical-monkey",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:46:08.980024Z",
     "start_time": "2021-10-09T00:46:08.971047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Register the Doc extension 'author' (default None)\n",
    "Doc.set_extension('author', default=None)\n",
    "\n",
    "# Register the Doc extension 'book' (default None)\n",
    "Doc.set_extension('book', default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "european-literature",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:49:32.376789Z",
     "start_time": "2021-10-09T00:49:32.332895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. \n",
      " — 'Metamorphosis' by Franz Kafka \n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing. \n",
      " — 'Moby-Dick or, The Whale' by Herman Melville \n",
      "\n",
      "It was the best of times, it was the worst of times. \n",
      " — 'A Tale of Two Cities' by Charles Dickens \n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars. \n",
      " — 'On the Road' by Jack Kerouac \n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen. \n",
      " — '1984' by George Orwell \n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing. \n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context['book']\n",
    "    doc._.author = context['author']    \n",
    "    # Print the text and custom attribute data\n",
    "    print(doc.text, '\\n', \"— '{}' by {}\".format(doc._.book, doc._.author), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-legislation",
   "metadata": {},
   "source": [
    "### Selective processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "stopped-friendly",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:50:40.371641Z",
     "start_time": "2021-10-09T00:50:40.354687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    "\n",
    "# Only tokenize the text\n",
    "doc = nlp.make_doc(text)\n",
    "\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "vertical-sensitivity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:51:38.212103Z",
     "start_time": "2021-10-09T00:51:38.193151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chick, American, College Park, Georgia)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'Chick'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token '-'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'fil'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'A'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'is'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'an'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'American'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'fast'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'food'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'restaurant'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'chain'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'headquartered'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'in'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'city'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'of'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'College'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'Park'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'Georgia'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'specializing'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'chicken'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'sandwiches'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "C:\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n"
     ]
    }
   ],
   "source": [
    "text = \"Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    "\n",
    "# Disable the tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-fifth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-elements",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-tucson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-brain",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
